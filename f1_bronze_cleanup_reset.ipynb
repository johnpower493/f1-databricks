{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090e4674-0fd4-4e9e-98ff-e43e3618ed6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üßπ F1 Bronze Cleanup & Reset Utility (Databricks)\n",
    "\n",
    "This notebook **safely drops and deletes** all F1 bronze Delta tables and their underlying data paths so you can re-run your extraction from a clean state.\n",
    "\n",
    "It is intended for use in **Databricks** with a Unity Catalog (e.g. `workspace.f1_bronze.session_results`) or legacy metastore (no catalog).\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "For each configured table name (e.g. `events`, `laps`, `session_results`, etc.) it will:\n",
    "\n",
    "1. Build the **fully-qualified table name** (e.g. `workspace.f1_bronze.session_results`).\n",
    "2. Check whether the table exists.\n",
    "3. If it exists:\n",
    "   - Run `DESCRIBE DETAIL` to find the underlying Delta path.\n",
    "   - `DROP TABLE` from the metastore.\n",
    "   - Recursively delete the **data path** using `dbutils.fs.rm(path, recurse=True)`.\n",
    "4. Log everything so you can see exactly what was done.\n",
    "\n",
    "> ‚ö†Ô∏è **Warning:** This will permanently delete the underlying Delta data files for the selected tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4dbe89f-79b1-477a-ad45-c9cc38a40270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------------\n",
    "# Adjust these to match your environment.\n",
    "\n",
    "# If you're using Unity Catalog, set your catalog name here, e.g. \"workspace\"\n",
    "# If you're NOT using Unity Catalog, set this to None.\n",
    "CATALOG = \"workspace\"  # or None\n",
    "\n",
    "# Schema / database that holds your F1 bronze tables\n",
    "SCHEMA = \"f1_bronze\"\n",
    "\n",
    "# Bronze tables you want to clean up.\n",
    "# The script will try each of these in turn.\n",
    "BRONZE_TABLES = [\n",
    "    \"events\",\n",
    "    \"laps\",\n",
    "    \"session_results\",\n",
    "    \"weather\",\n",
    "    \"track_status\",\n",
    "    \"race_control_messages\",\n",
    "    \"telemetry_raw\",\n",
    "    \"car_position\",\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Config loaded\")\n",
    "print(f\"  CATALOG = {CATALOG!r}\")\n",
    "print(f\"  SCHEMA  = {SCHEMA!r}\")\n",
    "print(f\"  TABLES  = {BRONZE_TABLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6c695b-6587-4a41-851b-4c4c8fda6cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def fq_table_name(table: str) -> str:\n",
    "    \"\"\"Build a fully qualified table name from CATALOG, SCHEMA, and table.\"\"\"\n",
    "    if CATALOG and len(CATALOG.strip()) > 0:\n",
    "        return f\"{CATALOG}.{SCHEMA}.{table}\"\n",
    "    else:\n",
    "        return f\"{SCHEMA}.{table}\"\n",
    "\n",
    "\n",
    "def table_exists(fq_name: str) -> bool:\n",
    "    \"\"\"Return True if the table exists in the current Spark catalog.\"\"\"\n",
    "    try:\n",
    "        return spark.catalog.tableExists(fq_name)\n",
    "    except Exception:\n",
    "        # Some Spark versions require (db, table) instead of fq string; fall back\n",
    "        try:\n",
    "            parts = fq_name.split(\".\")\n",
    "            if len(parts) == 3:\n",
    "                return spark.catalog.tableExists(f\"{parts[0]}.{parts[1]}\", parts[2])\n",
    "            elif len(parts) == 2:\n",
    "                return spark.catalog.tableExists(parts[0], parts[1])\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_table_path(fq_name: str) -> Optional[str]:\n",
    "    \"\"\"Return the underlying Delta path for a table via DESCRIBE DETAIL, or None.\"\"\"\n",
    "    try:\n",
    "        detail_df = spark.sql(f\"DESCRIBE DETAIL {fq_name}\")\n",
    "        detail = detail_df.collect()[0].asDict()\n",
    "        return detail.get(\"location\") or detail.get(\"path\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not DESCRIBE DETAIL {fq_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def drop_table(fq_name: str):\n",
    "    \"\"\"Drop the table from the metastore if it exists.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {fq_name}\")\n",
    "        print(f\"   ‚úÖ Dropped table {fq_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error dropping table {fq_name}: {e}\")\n",
    "\n",
    "\n",
    "def delete_path(path: str):\n",
    "    \"\"\"Delete a DBFS / cloud filesystem path using dbutils.fs.rm.\"\"\"\n",
    "    try:\n",
    "        if not path:\n",
    "            return\n",
    "        print(f\"   üóëÔ∏è Deleting path: {path}\")\n",
    "        dbutils.fs.rm(path, recurse=True)\n",
    "        print(f\"   ‚úÖ Deleted path: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting path {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bc8d95-2282-4109-a7f0-a22f7ee14466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAIN CLEANUP\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nüßπ Starting F1 bronze cleanup...\\n\")\n",
    "\n",
    "for tbl in BRONZE_TABLES:\n",
    "    fq_name = fq_table_name(tbl)\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    print(f\"üîé Checking table: {fq_name}\")\n",
    "    \n",
    "    if not table_exists(fq_name):\n",
    "        print(f\"   ‚ÑπÔ∏è Table does not exist, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Try to grab the underlying data path before dropping\n",
    "    path = get_table_path(fq_name)\n",
    "    if path:\n",
    "        print(f\"   üìÅ Data path: {path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No path found via DESCRIBE DETAIL (may be a view or non-Delta table).\")\n",
    "\n",
    "    # Drop the table\n",
    "    drop_table(fq_name)\n",
    "\n",
    "    # If we found a location, try to delete it\n",
    "    if path:\n",
    "        delete_path(path)\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete. You can now re-run your F1 bronze extraction notebook from a clean state.\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "f1_bronze_cleanup_reset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
